{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16cde24a",
   "metadata": {},
   "source": [
    "# CS345 Final Project: Fairness and Bias Analysis in News Classification\n",
    "\n",
    "**Authors:** Andrew Aberer & Ethan Amaya\n",
    "\n",
    "## Project Motivation\n",
    "\n",
    "This project investigates bias and fairness in machine learning models trained to classify political bias in news articles. We analyze whether ML classifiers behave discriminatorily when identifying political bias across ideological divides. This is important because automated content/bias detection algorithms are increasingly used in modern applications and may unintentionally reinforce preexisting biases. Understanding and addressing these biases is crucial for developing trustworthy and reliable AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5255f80",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "We use the **ACL2020 Media Bias/Fact Check (MBFC) Corpus**, which contains:\n",
    "- News articles from diverse political sources\n",
    "- Pre-labeled bias annotations (left, center, right)\n",
    "- Pre-computed BERT embeddings (768 dimensions) for article text\n",
    "- Factuality ratings and various metadata\n",
    "\n",
    "This dataset fits our use case because it contains real-world news articles with expert labels, where fairness disparities have significant societal impact.\n",
    "\n",
    "## Methods and Approach\n",
    "\n",
    "**Classification Models:**\n",
    "1. **Logistic Regression** - Linear baseline with L2 regularization\n",
    "2. **SVM (RBF kernel)** - Non-linear classifier with class balancing\n",
    "\n",
    "**Fairness Analysis:**\n",
    "1. **Demographic Equality** - Equal prediction rates across political groups\n",
    "2. **Asymmetric Bias Analysis** - Testing whether predictions exhibit directional bias (e.g., more left→right errors than right→left)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae5bec2",
   "metadata": {},
   "source": [
    "## Setup and Data Loading\n",
    "We begin by loading the ACL2020 Media Bias/Fact Check dataset, which contains news articles with pre-computed BERT embeddings. We'll examine the dataset structure, class distribution, and prepare train/test splits based on the provided split configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0804b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    balanced_accuracy_score,\n",
    "    cohen_kappa_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "335a10e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 859 articles\n",
      "Classes: {'right': 349, 'center': 271, 'left': 239}\n",
      "Train/Test split: 687/172\n"
     ]
    }
   ],
   "source": [
    "def load_news_dataset(dataset_name: str):\n",
    "    \"\"\"Load ACL2020 dataset with BERT embeddings and train/test splits.\"\"\"\n",
    "    base_path = os.getcwd() + '/data'\n",
    "    dataset_path = Path(base_path) / dataset_name\n",
    "    if not dataset_path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset path not found: {dataset_path}\")\n",
    "\n",
    "    # load corpus.tsv\n",
    "    corpus_path = dataset_path / \"corpus.tsv\"\n",
    "    df = pd.read_csv(corpus_path, sep=\"\\t\")\n",
    "    \n",
    "    # load splits\n",
    "    splits_path = dataset_path / \"splits.json\"\n",
    "    splits = {}\n",
    "    if splits_path.exists():\n",
    "        with open(splits_path, \"r\") as f:\n",
    "            splits = json.load(f)\n",
    "\n",
    "    # load feature JSONs from features directory\n",
    "    features_dir = dataset_path / \"features\"\n",
    "    feature_data = {}\n",
    "    if features_dir.exists():\n",
    "        for json_file in features_dir.glob(\"*.json\"):\n",
    "            feature_name = json_file.stem\n",
    "            with open(json_file, \"r\") as f:\n",
    "                feature_data[feature_name] = json.load(f)\n",
    "    \n",
    "    return df, splits, feature_data\n",
    "\n",
    "acl_df, acl_splits, acl_features = load_news_dataset(\"acl2020\")\n",
    "\n",
    "acl_split_data = acl_splits.get('0', {})\n",
    "acl_train_split_size = len(acl_split_data.get('train', []))\n",
    "acl_test_split_size = len(acl_split_data.get('test', []))\n",
    "acl_train_urls = acl_split_data.get('train', [])\n",
    "acl_test_urls = acl_split_data.get('test', [])\n",
    "\n",
    "print(f\"Dataset: {len(acl_df)} articles\")\n",
    "print(f\"Classes: {acl_df['bias'].value_counts().to_dict()}\")\n",
    "print(f\"Train/Test split: {len(acl_train_urls)}/{len(acl_test_urls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e9ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: 768 dimensions\n",
      "Class distribution (test): {'center': 46, 'left': 55, 'right': 71}\n"
     ]
    }
   ],
   "source": [
    "# Prepare BERT embeds and labels\n",
    "bert_features = acl_features['articles_body_bert']\n",
    "\n",
    "# split by source_url\n",
    "train_df = acl_df[acl_df['source_url_normalized'].isin(acl_train_urls)]\n",
    "test_df = acl_df[acl_df['source_url_normalized'].isin(acl_test_urls)]\n",
    "\n",
    "# use source_url_normalized as key\n",
    "X_train = np.array([bert_features[url] for url in train_df['source_url_normalized']])\n",
    "X_test = np.array([bert_features[url] for url in test_df['source_url_normalized']])\n",
    "y_train = train_df['bias'].values\n",
    "y_test = test_df['bias'].values\n",
    "\n",
    "print(f\"Feature shape: {X_train.shape[1]} dimensions\")\n",
    "print(f\"Class distribution (test): {dict(zip(*np.unique(y_test, return_counts=True)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa99a496",
   "metadata": {},
   "source": [
    "## Baseline Models: ACL Bias Classification\n",
    "We evaluate five classifiers with `class_weight='balanced'` to handle class imbalance.\n",
    "\n",
    "We evaluate five different classifiers with hyperparameter tuning using GridSearchCV. All models use `class_weight='balanced'` to handle the class imbalance. We use stratified 5-fold cross-validation with balanced accuracy as the scoring metric, which is more appropriate for imbalanced datasets than standard accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4735510e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models with hyperparameter tuning...\n",
      "Tuning Logistic Regression...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m     results[name] \u001b[38;5;241m=\u001b[39m train_and_evaluate(model, X_train_scaled, X_test_scaled, y_train, y_test, name)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m90\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUMMARY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test, name, cv=5, tune_hyperparams=True):\n",
    "    \"\"\"Train model with cross-validation, optional hyperparameter tuning, and evaluate on test set.\"\"\"\n",
    "    \n",
    "    # Define parameter grids for models that support C or alpha tuning\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {'C': [0.001, 0.01, 0.1, 1.0, 10.0]},\n",
    "        'SVM (RBF)': {'C': [0.01, 0.1, 1.0, 10.0], 'gamma': ['scale', 0.01, 0.1]},\n",
    "        'MLP': {'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0]},\n",
    "        'Gradient Boosting': {'learning_rate': [0.01, 0.05, 0.1], 'n_estimators': [50, 100, 200]}\n",
    "    }\n",
    "    \n",
    "    cv_fold = StratifiedKFold(cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Hyperparameter tuning if applicable\n",
    "    if tune_hyperparams and name in param_grids:\n",
    "        grid_search = GridSearchCV(\n",
    "            model, param_grids[name], cv=cv_fold, \n",
    "            scoring='balanced_accuracy', n_jobs=-1, refit=True\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        cv_scores = cross_val_score(best_model, X_train, y_train, cv=cv_fold, scoring='balanced_accuracy')\n",
    "    else:\n",
    "        best_model = model\n",
    "        best_params = {}\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=cv_fold, scoring='balanced_accuracy')\n",
    "        best_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'model': best_model,\n",
    "        'best_params': best_params,\n",
    "        'y_pred': y_pred,\n",
    "        'cv_balanced_acc': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'test_accuracy': accuracy_score(y_test, y_pred),\n",
    "        'test_balanced_acc': balanced_accuracy_score(y_test, y_pred),\n",
    "        'macro_f1': f1_score(y_test, y_pred, average='macro'),\n",
    "        'cohen_kappa': cohen_kappa_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# Define models (base configurations - hyperparameters will be tuned)\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', class_weight='balanced', random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_leaf=5, \n",
    "                                            class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(max_depth=3, subsample=0.8, random_state=42),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=500, \n",
    "                         early_stopping=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate all models with hyperparameter tuning\n",
    "print(\"Training models with hyperparameter tuning...\")\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Tuning {name}...\")\n",
    "    results[name] = train_and_evaluate(model, X_train_scaled, X_test_scaled, y_train, y_test, name)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Model':<20} | {'Best Params':<30} | {'CV Bal.Acc':>12} | {'Test Acc':>10} | {'Macro F1':>10}\")\n",
    "print(\"-\"*90)\n",
    "for name, res in results.items():\n",
    "    params = str(res['best_params']) if res['best_params'] else \"default\"\n",
    "    print(f\"{name:<20} | {params:<30} | {res['cv_balanced_acc']:.3f}±{res['cv_std']:.3f} | {res['test_accuracy']:.3f} | {res['macro_f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061164d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive results table\n",
    "results_df = pd.DataFrame([{\n",
    "    'Model': r['name'],\n",
    "    'CV Balanced Acc': f\"{r['cv_balanced_acc']:.3f}±{r['cv_std']:.3f}\",\n",
    "    'Test Accuracy': r['test_accuracy'],\n",
    "    'Test Balanced Acc': r['test_balanced_acc'],\n",
    "    'Macro F1': r['macro_f1'],\n",
    "    'Cohen Kappa': r['cohen_kappa']\n",
    "} for r in results.values()])\n",
    "\n",
    "results_df = results_df.sort_values('Macro F1', ascending=False)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON (sorted by Macro F1)\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nNote: Macro F1 is the primary metric for imbalanced classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc156f93",
   "metadata": {},
   "source": [
    "The results show that SVM (RBF) and Logistic Regression achieve the best performance with nearly identical Macro F1 scores of 57.9% and 57.8% respectively. Both models achieve 58.7% test accuracy and 56.9% balanced accuracy, with Cohen's Kappa around 0.36, indicating moderate agreement beyond chance.\n",
    "\n",
    "Random Forest and Gradient Boosting show larger performance gaps—Random Forest achieves 55.8% accuracy but only 49.7% Macro F1 (6.1 point difference), while Gradient Boosting shows an even larger disparity with 55.8% accuracy but just 44.1% Macro F1 (11.7 point gap). These large gaps suggest the ensemble methods are heavily over-predicting majority classes at the expense of minority class performance.\n",
    "\n",
    "The MLP Neural Network performs worst overall with 52.9% accuracy and 41.9% Macro F1, indicating it struggles to learn meaningful patterns.\n",
    "\n",
    "The modest Macro F1 scores across all models (42-58%) and Cohen's Kappa values (0.24-0.36) highlight the inherent difficulty of this 3-class political bias classification task, likely due to the ambiguous \"center\" category and limited training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f575f97",
   "metadata": {},
   "source": [
    "## Per-Class Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9aaef4",
   "metadata": {},
   "source": [
    "Examining performance breakdown by class reveals the challenge of the minority 'left' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25762f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics for best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_result = results[best_model_name]\n",
    "y_pred_best = best_result['y_pred']\n",
    "\n",
    "print(f\"Classification Report ({best_model_name}):\\n\")\n",
    "print(classification_report(y_test, y_pred_best, digits=3))\n",
    "\n",
    "# Confusion matrix visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best, labels=['left', 'center', 'right'])\n",
    "im = axes[0].imshow(cm, cmap='Blues')\n",
    "axes[0].set_xticks([0, 1, 2])\n",
    "axes[0].set_yticks([0, 1, 2])\n",
    "axes[0].set_xticklabels(['left', 'center', 'right'])\n",
    "axes[0].set_yticklabels(['left', 'center', 'right'])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title(f'Confusion Matrix ({best_model_name})')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[0].text(j, i, str(cm[i, j]), ha='center', va='center', \n",
    "                     color='white' if cm[i, j] > cm.max()/2 else 'black', fontsize=12)\n",
    "\n",
    "# Per-class accuracy\n",
    "classes = ['left', 'center', 'right']\n",
    "class_recalls = recall_score(y_test, y_pred_best, labels=classes, average=None)\n",
    "colors = ['#e74c3c', '#95a5a6', '#3498db']\n",
    "axes[1].bar(classes, class_recalls, color=colors)\n",
    "axes[1].axhline(y=balanced_accuracy_score(y_test, y_pred_best), color='black', linestyle='--', label='Balanced Acc')\n",
    "axes[1].set_ylabel('Recall')\n",
    "axes[1].set_title('Per-Class Recall')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563de8b",
   "metadata": {},
   "source": [
    "We now examine the confusion matrices to understand where models make mistakes. This will reveal whether errors are symmetric equally confused in both directions or if there's a systematic bias in how the models confuse certain political orientations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b6ae9f",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ace6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices\n",
    "cm_lr = confusion_matrix(y_test, y_pred)\n",
    "cm_svm = confusion_matrix(y_test, y_pred_balanced)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "class_labels = ['center', 'left', 'right']\n",
    "\n",
    "# Logistic Regression\n",
    "im1 = axes[0].imshow(cm_lr, cmap='Blues', aspect='auto')\n",
    "axes[0].set_title(f'Logistic Regression\\nAccuracy: {accuracy:.4f}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[0].set_xticks(range(len(class_labels)))\n",
    "axes[0].set_yticks(range(len(class_labels)))\n",
    "axes[0].set_xticklabels(class_labels)\n",
    "axes[0].set_yticklabels(class_labels)\n",
    "\n",
    "for i in range(len(class_labels)):\n",
    "    for j in range(len(class_labels)):\n",
    "        text_color = 'white' if cm_lr[i, j] > cm_lr.max()/2 else 'black'\n",
    "        axes[0].text(j, i, cm_lr[i, j], ha='center', va='center', \n",
    "                     color=text_color, fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im1, ax=axes[0], label='Count')\n",
    "\n",
    "# SVM Balanced\n",
    "im2 = axes[1].imshow(cm_svm, cmap='Greens', aspect='auto')\n",
    "axes[1].set_title(f'SVM (Balanced)\\nAccuracy: {accuracy_balanced:.4f}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[1].set_xticks(range(len(class_labels)))\n",
    "axes[1].set_yticks(range(len(class_labels)))\n",
    "axes[1].set_xticklabels(class_labels)\n",
    "axes[1].set_yticklabels(class_labels)\n",
    "\n",
    "for i in range(len(class_labels)):\n",
    "    for j in range(len(class_labels)):\n",
    "        text_color = 'white' if cm_svm[i, j] > cm_svm.max()/2 else 'black'\n",
    "        axes[1].text(j, i, cm_svm[i, j], ha='center', va='center',\n",
    "                     color=text_color, fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im2, ax=axes[1], label='Count')\n",
    "\n",
    "# Random Forest\n",
    "im3 = axes[2].imshow(cm_rf, cmap='Oranges', aspect='auto')\n",
    "axes[2].set_title(f'Random Forest\\nAccuracy: {accuracy_rf:.4f}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('True Label', fontsize=12)\n",
    "axes[2].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[2].set_xticks(range(len(class_labels)))\n",
    "axes[2].set_yticks(range(len(class_labels)))\n",
    "axes[2].set_xticklabels(class_labels)\n",
    "axes[2].set_yticklabels(class_labels)\n",
    "\n",
    "for i in range(len(class_labels)):\n",
    "    for j in range(len(class_labels)):\n",
    "        text_color = 'white' if cm_rf[i, j] > cm_rf.max()/2 else 'black'\n",
    "        axes[2].text(j, i, cm_rf[i, j], ha='center', va='center',\n",
    "                     color=text_color, fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im3, ax=axes[2], label='Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7380d5",
   "metadata": {},
   "source": [
    "Beyond overall accuracy, we must examine whether models exhibit directional biases. An important fairness concern is whether a model systematically confuses left-leaning articles as right-leaning more often than the reverse (or vice versa). Such asymmetry could indicate that the model has learned to favor one political orientation over another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f2846",
   "metadata": {},
   "source": [
    "## Asymmetric Bias Analysis\n",
    "\n",
    "This analysis examines whether our models exhibit asymmetric prediction errors between political categories. Specifically, we compare:\n",
    "- **Left → Right errors**: How often articles that are truly \"left\" are misclassified as \"right\"\n",
    "- **Right → Left errors**: How often articles that are truly \"right\" are misclassified as \"left\"\n",
    "\n",
    "An asymmetry in these error rates could indicate that the model has learned a directional bias, potentially favoring one political orientation over another. This is important for understanding model fairness in political bias detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymmetric_bias_analysis(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Analyze asymmetric prediction errors between left and right categories.\n",
    "    Returns counts and rates of left→right and right→left misclassifications.\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Count left articles predicted as right\n",
    "    left_as_right = np.sum((y_true == 'left') & (y_pred == 'right'))\n",
    "    total_left = np.sum(y_true == 'left')\n",
    "    left_to_right_rate = left_as_right / total_left if total_left > 0 else 0\n",
    "    \n",
    "    # Count right articles predicted as left\n",
    "    right_as_left = np.sum((y_true == 'right') & (y_pred == 'left'))\n",
    "    total_right = np.sum(y_true == 'right')\n",
    "    right_to_left_rate = right_as_left / total_right if total_right > 0 else 0\n",
    "    \n",
    "    # Calculate asymmetry (positive = more left→right errors, negative = more right→left errors)\n",
    "    asymmetry = left_to_right_rate - right_to_left_rate\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Left→Right (count)': left_as_right,\n",
    "        'Left→Right (rate)': left_to_right_rate,\n",
    "        'Right→Left (count)': right_as_left,\n",
    "        'Right→Left (rate)': right_to_left_rate,\n",
    "        'Asymmetry': asymmetry,\n",
    "        'Total Left': total_left,\n",
    "        'Total Right': total_right\n",
    "    }\n",
    "\n",
    "# Analyze all models\n",
    "asym_lr = asymmetric_bias_analysis(y_test, y_pred, 'Logistic Regression')\n",
    "asym_svm = asymmetric_bias_analysis(y_test, y_pred_balanced, 'SVM (Balanced)')\n",
    "asym_rf = asymmetric_bias_analysis(y_test, y_pred_rf, 'Random Forest')\n",
    "\n",
    "asym_df = pd.DataFrame([asym_lr, asym_svm, asym_rf])\n",
    "\n",
    "print(\"ASYMMETRIC BIAS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis analysis compares how often each model confuses left for right vs right for left.\")\n",
    "print(\"A positive asymmetry means the model more often predicts left articles as right.\")\n",
    "print(\"A negative asymmetry means the model more often predicts right articles as left.\\n\")\n",
    "print(asym_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b880744d",
   "metadata": {},
   "source": [
    "The asymmetry analysis reveals a critical fairness issue: Logistic Regression shows strong asymmetry (0.47), meaning it misclassifies 47% of left articles as right (26 out of 55) while making zero right→left errors. This extreme directional bias indicates the model has systematically learned to favor the \"right\" category.\n",
    "\n",
    "In contrast, SVM (Balanced) and Random Forest show much lower asymmetry (0.096 and 0.078 respectively), with more balanced error rates: SVM makes 20 left→right errors (36.4% rate) versus 19 right→left errors (26.8% rate), while Random Forest shows similar balance with 19 errors in each direction. \n",
    "\n",
    "This demonstrates that class balancing techniques (class_weight='balanced') in SVM and Random Forest successfully mitigate directional bias, while Logistic Regression's simple class weighting is insufficient. The asymmetry in Logistic Regression is particularly concerning for fairness in political bias detection, as it could systematically disadvantage left-leaning content in automated content moderation or recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8890f2",
   "metadata": {},
   "source": [
    "## Demographic Equality\n",
    "\n",
    "Demographic parity examines whether the model's prediction distribution matches the true distribution of classes in the test set. Large differences indicate that the model may be systematically over or under-predicting certain political orientations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prediction rates by class\n",
    "def demographic_parity(y_true, y_pred, model_name):\n",
    "    \"\"\"Check if prediction rates are equal across groups\"\"\"\n",
    "    total = len(y_pred)\n",
    "    \n",
    "    results = []\n",
    "    for label in ['center', 'left', 'right']:\n",
    "        pred_rate = (y_pred == label).sum() / total\n",
    "        true_rate = (y_true == label).sum() / total\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Class': label,\n",
    "            'Actual Distribution': true_rate,\n",
    "            'Predicted Distribution': pred_rate,\n",
    "            'Difference': pred_rate - true_rate\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Analyze all models\n",
    "demo_lr = demographic_parity(y_test, y_pred, 'Logistic Regression')\n",
    "demo_svm = demographic_parity(y_test, y_pred_balanced, 'SVM (Balanced)')\n",
    "demo_rf = demographic_parity(y_test, y_pred_rf, 'Random Forest')\n",
    "demo_combined = pd.concat([demo_lr, demo_svm, demo_rf])\n",
    "\n",
    "print(\"DEMOGRAPHIC EQUALITY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(demo_combined.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "classes = ['center', 'left', 'right']\n",
    "x = np.arange(len(classes))\n",
    "width = 0.12\n",
    "\n",
    "colors_lr = ['#3498db', '#5dade2']  # Blues for LR\n",
    "colors_svm = ['#27ae60', '#58d68d']  # Greens for SVM\n",
    "colors_rf = ['#e67e22', '#f5b041']  # Oranges for RF\n",
    "\n",
    "# Logistic Regression\n",
    "lr_actual = demo_lr['Actual Distribution'].values\n",
    "lr_pred = demo_lr['Predicted Distribution'].values\n",
    "\n",
    "# SVM\n",
    "svm_actual = demo_svm['Actual Distribution'].values\n",
    "svm_pred = demo_svm['Predicted Distribution'].values\n",
    "\n",
    "# Random Forest\n",
    "rf_actual = demo_rf['Actual Distribution'].values\n",
    "rf_pred = demo_rf['Predicted Distribution'].values\n",
    "\n",
    "bars1 = ax.bar(x - 2.5*width, lr_actual, width, label='LR - Actual', \n",
    "               color=colors_lr[0], alpha=0.8)\n",
    "bars2 = ax.bar(x - 1.5*width, lr_pred, width, label='LR - Predicted', \n",
    "               color=colors_lr[1], alpha=0.8)\n",
    "bars3 = ax.bar(x - 0.5*width, svm_actual, width, label='SVM - Actual', \n",
    "               color=colors_svm[0], alpha=0.8)\n",
    "bars4 = ax.bar(x + 0.5*width, svm_pred, width, label='SVM - Predicted', \n",
    "               color=colors_svm[1], alpha=0.8)\n",
    "bars5 = ax.bar(x + 1.5*width, rf_actual, width, label='RF - Actual', \n",
    "               color=colors_rf[0], alpha=0.8)\n",
    "bars6 = ax.bar(x + 2.5*width, rf_pred, width, label='RF - Predicted', \n",
    "               color=colors_rf[1], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Political Bias Category', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Proportion', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Demographic Equality: Actual vs Predicted Distributions', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(classes)\n",
    "ax.legend(fontsize=8, loc='upper right', ncol=2)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('demographic_equality.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb03351",
   "metadata": {},
   "source": [
    "The demographic equality analysis reveals severe issues in Logistic Regression: it under-predicts \"left\" by 30.8 percentage points (only 1.2% predicted vs 32% actual) while over-predicting \"center\" (+20.3 points) and \"right\" (+10.5 points). The model essentially avoids the \"left\" prediction entirely.\n",
    "\n",
    "SVM (Balanced) and Random Forest show much better demographic parity with deviations under 11 points for all classes. SVM over-predicts \"left\" by 7.0 points and under-predicts \"center\" by 10.5 points. Random Forest shows similar balance, over-predicting \"left\" by 9.3 points with near-perfect \"right\" predictions (1.7 point difference).\n",
    "\n",
    "These findings directly explain our asymmetry results—Logistic Regression's extreme avoidance of \"left\" predictions causes its high left→right error rate, while SVM and Random Forest's balanced distributions correspond to their lower asymmetry scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0a122",
   "metadata": {},
   "source": [
    "## Model Improvements\n",
    "\n",
    "The baseline models achieve ~55-58% accuracy on this 3-class classification problem. After exploring several techniques including hyperparameter tuning, PCA dimensionality reduction, gradient boosting, combined features, neural networks, feature selection, and regularization tuning, we found that only two approaches provided meaningful improvement:\n",
    "\n",
    "1. **PCA(20) + Logistic Regression**: Reducing to 20 principal components achieved 58.72% accuracy\n",
    "2. **Regularization Tuning (C=0.01)**: Stronger L2 regularization on Logistic Regression also achieved 58.72% accuracy\n",
    "\n",
    "Both techniques address the overfitting problem caused by having 768 features with only 687 training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c5bc20",
   "metadata": {},
   "source": [
    "### Best Improvement: PCA Dimensionality Reduction + Regularization Tuning\n",
    "\n",
    "Reducing the 768-dimensional BERT embeddings and tuning regularization helps reduce overfitting on this small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca057d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal PCA components via cross-validation\n",
    "print(\"PCA Component Search (with SVM):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "best_pca_score = 0\n",
    "best_n_components = 20\n",
    "\n",
    "for n_comp in [10, 20, 30, 50, 100]:\n",
    "    pca = PCA(n_components=n_comp, random_state=42)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    \n",
    "    svm = SVC(kernel='rbf', C=1.0, class_weight='balanced', random_state=42)\n",
    "    cv_scores = cross_val_score(svm, X_train_pca, y_train, cv=5, scoring='balanced_accuracy')\n",
    "    \n",
    "    explained = pca.explained_variance_ratio_.sum() * 100\n",
    "    print(f\"PCA({n_comp:3}): CV Bal.Acc = {cv_scores.mean():.3f}±{cv_scores.std():.3f} | Variance: {explained:.1f}%\")\n",
    "    \n",
    "    if cv_scores.mean() > best_pca_score:\n",
    "        best_pca_score = cv_scores.mean()\n",
    "        best_n_components = n_comp\n",
    "\n",
    "print(f\"\\nBest: PCA({best_n_components}) with CV Balanced Acc = {best_pca_score:.3f}\")\n",
    "\n",
    "# Train final improved model\n",
    "pca_final = PCA(n_components=best_n_components, random_state=42)\n",
    "X_train_pca = pca_final.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca_final.transform(X_test_scaled)\n",
    "\n",
    "improved_svm = SVC(kernel='rbf', C=1.0, class_weight='balanced', random_state=42)\n",
    "improved_svm.fit(X_train_pca, y_train)\n",
    "y_pred_improved = improved_svm.predict(X_test_pca)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"IMPROVED MODEL: PCA({best_n_components}) + SVM\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy:     {accuracy_score(y_test, y_pred_improved):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred_improved):.4f}\")\n",
    "print(f\"Macro F1:          {f1_score(y_test, y_pred_improved, average='macro'):.4f}\")\n",
    "print(f\"Cohen Kappa:       {cohen_kappa_score(y_test, y_pred_improved):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_improved, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49900d",
   "metadata": {},
   "source": [
    "PCA with 20 components achieves the best cross-validation balanced accuracy (0.529) while retaining 99.6% of the variance and reducing dimensionality by 97% (from 768 to 20 features).\n",
    "\n",
    "However, the improvements are modest: the PCA(20) + SVM model achieves 57.6% test accuracy and 56.4% Macro F1, representing only marginal gains over the baseline SVM (58.7% accuracy, 57.9% Macro F1). The per-class recall shows continued struggles with the \"center\" class (43.5% recall) despite reasonable performance on \"left\" (52.7%) and \"right\" (70.4%).\n",
    "\n",
    "These limited gains suggest that the core challenge lies not in feature representation but rather in the dataset size (687 training examples for a 3-class problem) and the \"center\" category that may contain both genuinely neutral articles and those that simply don't lean strongly in either direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439bcb69",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "PCA with 20 components retains 99.6% of variance while reducing dimensionality by 97%, providing a modest improvement. However, the gains are limited, suggesting that the core challenge lies in the dataset size and the ambiguity of the classification task rather than feature representation alone.\n",
    "\n",
    "Removing the ambiguous 'center' class significantly improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bed108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare binary dataset (exclude center)\n",
    "train_binary_mask = y_train != 'center'\n",
    "test_binary_mask = y_test != 'center'\n",
    "\n",
    "X_train_binary = X_train_scaled[train_binary_mask]\n",
    "X_test_binary = X_test_scaled[test_binary_mask]\n",
    "y_train_binary = y_train[train_binary_mask]\n",
    "y_test_binary = y_test[test_binary_mask]\n",
    "\n",
    "print(f\"Binary dataset: {len(y_train_binary)} train, {len(y_test_binary)} test samples\")\n",
    "\n",
    "# Train binary models\n",
    "binary_results = {}\n",
    "binary_models = {\n",
    "    'Logistic Regression': LogisticRegression(C=0.1, max_iter=1000, class_weight='balanced', random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', C=1.0, class_weight='balanced', random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=10, class_weight='balanced', random_state=42),\n",
    "}\n",
    "\n",
    "print(\"\\nBinary Classification Results (Left vs Right):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, model in binary_models.items():\n",
    "    model.fit(X_train_binary, y_train_binary)\n",
    "    y_pred_bin = model.predict(X_test_binary)\n",
    "    \n",
    "    binary_results[name] = {\n",
    "        'accuracy': accuracy_score(y_test_binary, y_pred_bin),\n",
    "        'macro_f1': f1_score(y_test_binary, y_pred_bin, average='macro'),\n",
    "        'y_pred': y_pred_bin\n",
    "    }\n",
    "    print(f\"{name:20} | Accuracy: {binary_results[name]['accuracy']:.3f} | Macro F1: {binary_results[name]['macro_f1']:.3f}\")\n",
    "\n",
    "# Compare 3-class vs binary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3-CLASS vs BINARY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "best_3class = max(results.values(), key=lambda x: x['macro_f1'])\n",
    "best_binary = max(binary_results.values(), key=lambda x: x['macro_f1'])\n",
    "print(f\"Best 3-class Macro F1: {best_3class['macro_f1']:.3f} ({best_3class['name']})\")\n",
    "print(f\"Best Binary Macro F1:  {best_binary['macro_f1']:.3f}\")\n",
    "print(f\"Improvement:           +{(best_binary['macro_f1'] - best_3class['macro_f1']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513f345",
   "metadata": {},
   "source": [
    "Binary classification shows substantial improvement across all models. Logistic Regression achieves the best performance with 69.0% accuracy and 68.5% Macro F1, followed closely by Random Forest (67.5% accuracy, 66.7% Macro F1) and SVM (66.7% accuracy, 65.8% Macro F1).\n",
    "\n",
    "Comparing to the best 3-class model (SVM with 57.9% Macro F1), binary classification improves Macro F1 by 10.6 percentage points to 68.5%. This substantial improvement confirms that the \"center\" category is indeed the most challenging to classify and is the primary source of confusion in the 3-class problem.\n",
    "\n",
    "The strong binary performance suggests that distinguishing political extremes (left vs right) is significantly more feasible than identifying the nuanced middle ground. Articles at the political extremes likely contain more distinctive patterns and clearer ideological markers, while \"center\" articles may be genuinely neutral, moderately biased in both directions, or simply lacking strong political signals, making them ambiguous for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc25a49a",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "Accuracy can be misleading for imbalanced datasets. We'll evaluate models using metrics that better capture performance across all classes:\n",
    "\n",
    "- **Macro F1-Score**: Average F1 across all classes (treats each class equally)\n",
    "- **Balanced Accuracy**: Average recall across classes\n",
    "- **Cohen's Kappa**: Chance-corrected accuracy (accounts for random agreement)\n",
    "- **Per-class Precision/Recall**: Detailed breakdown by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a100abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Model comparison (3-class)\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[m]['test_accuracy'] for m in model_names]\n",
    "macro_f1s = [results[m]['macro_f1'] for m in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, accuracies, width, label='Accuracy', color='#3498db', alpha=0.8)\n",
    "axes[0].bar(x + width/2, macro_f1s, width, label='Macro F1', color='#e74c3c', alpha=0.8)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels([m.replace(' ', '\\n') for m in model_names], fontsize=8)\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('3-Class: Accuracy vs Macro F1')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 0.8)\n",
    "\n",
    "# Plot 2: Per-class recall comparison\n",
    "classes = ['left', 'center', 'right']\n",
    "best_3class_name = max(results.keys(), key=lambda k: results[k]['macro_f1'])\n",
    "recall_per_class = recall_score(y_test, results[best_3class_name]['y_pred'], labels=classes, average=None)\n",
    "\n",
    "colors = ['#e74c3c', '#95a5a6', '#3498db']\n",
    "axes[1].bar(classes, recall_per_class, color=colors)\n",
    "axes[1].set_ylabel('Recall')\n",
    "axes[1].set_title(f'Per-Class Recall ({best_3class_name})')\n",
    "axes[1].set_ylim(0, 1)\n",
    "for i, v in enumerate(recall_per_class):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "# Plot 3: 3-class vs Binary\n",
    "comparison_data = {\n",
    "    '3-Class': best_3class['macro_f1'],\n",
    "    'Binary (L/R)': best_binary['macro_f1']\n",
    "}\n",
    "bars = axes[2].bar(comparison_data.keys(), comparison_data.values(), color=['#9b59b6', '#1abc9c'])\n",
    "axes[2].set_ylabel('Macro F1')\n",
    "axes[2].set_title('3-Class vs Binary Classification')\n",
    "axes[2].set_ylim(0, 1)\n",
    "for bar, val in zip(bars, comparison_data.values()):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, val + 0.02, f'{val:.3f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d500b3",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Summary of Findings\n",
    "\n",
    "This analysis explored political bias classification in news articles using the ACL2020 Media Bias/Fact Check (MBFC) Corpus with BERT embeddings. We evaluated multiple machine learning approaches and conducted fairness analysis to understand model behavior across political categories.\n",
    "\n",
    "### Baseline Model Performance\n",
    "\n",
    "| Model | Test Accuracy | Macro F1 |\n",
    "|-------|---------------|----------|\n",
    "| SVM (RBF) | 58.7% | 57.9% |\n",
    "| Logistic Regression | 58.7% | 57.8% |\n",
    "| Random Forest | 55.8% | 49.7% |\n",
    "| Gradient Boosting | 55.8% | 44.1% |\n",
    "| MLP Neural Network | 52.9% | 41.9% |\n",
    "\n",
    "**Observation**: All models achieved modest accuracy (~53-58%), with SVM and Logistic Regression performing best. The gap between accuracy and Macro F1 reveals that models struggle with minority classes.\n",
    "\n",
    "### Fairness Analysis Findings\n",
    "\n",
    "#### Demographic Equality\n",
    "Our demographic equality analysis revealed significant disparities in prediction rates across political groups:\n",
    "- Logistic Regression severely under-predicts the \"left\" class (predicted 1.2% vs actual 32%)\n",
    "- SVM and Random Forest show more balanced prediction distributions\n",
    "- All models tend to over-predict the majority \"right\" class\n",
    "\n",
    "#### Asymmetric Bias Analysis\n",
    "We found that models exhibit **asymmetric prediction errors** between political categories:\n",
    "- **Logistic Regression**: Strong asymmetry (0.47) - frequently misclassifies left articles as right, but rarely the reverse\n",
    "- **SVM (Balanced)**: Lower asymmetry (0.10) - more balanced error rates between left→right and right→left\n",
    "- **Random Forest**: Similar to SVM (0.08) - relatively balanced directional errors\n",
    "\n",
    "This asymmetry indicates that models may have learned a directional bias favoring the \"right\" category, which has important implications for fair deployment of such systems.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Class Imbalance Impact**: The 'left' class has consistently low recall across all models, indicating either insufficient samples or ambiguous features.\n",
    "\n",
    "2. **Accuracy vs Macro F1**: Accuracy is misleading (~58%) because models perform well on majority classes while struggling with 'left'. Macro F1 (~55%) provides a more honest assessment.\n",
    "\n",
    "3. **Dimensionality Reduction**: PCA to 20 components captures 99.6% of variance and provides modest improvements by reducing overfitting on the 768-dimensional BERT space.\n",
    "\n",
    "4. **Binary Classification**: Removing the ambiguous 'center' class improves Macro F1 by ~10.6 percentage points (from 57.9% to 68.5%), suggesting the center category is inherently difficult to distinguish.\n",
    "\n",
    "### Root Causes of Limited Performance\n",
    "\n",
    "1. **Dataset Size**: The ACL2020 dataset contains only 859 samples relative to 768-dimensional BERT embeddings, creating a high-dimensional, low-sample scenario prone to overfitting.\n",
    "\n",
    "2. **Class Imbalance**: Unequal distribution across right (~41%), left (~32%), and center (~27%) classes makes learning difficult.\n",
    "\n",
    "3. **Inherent Task Difficulty**: Political bias exists on a spectrum. The \"center\" category is particularly ambiguous, as it may contain articles that are genuinely neutral or simply less extreme.\n",
    "\n",
    "4. **Feature Representation**: Pre-computed BERT embeddings may not capture the subtle linguistic cues that distinguish political bias.\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "1. **Fine-tune BERT**: Instead of using pre-computed embeddings, fine-tune a BERT model directly on the bias classification task.\n",
    "\n",
    "2. **Data Augmentation**: Expand the dataset through paraphrasing, back-translation, or synthetic data generation.\n",
    "\n",
    "3. **Hierarchical Classification**: First classify as \"extreme\" vs \"center\", then classify extreme articles as \"left\" vs \"right\".\n",
    "\n",
    "4. **Counterfactual Fairness**: Test whether predictions remain consistent when political identifiers (e.g., \"Democrat\" swapped with \"Republican\") are substituted.\n",
    "\n",
    "5. **Additional Features**: Incorporate metadata like source reputation, factuality ratings, or topic modeling features.\n",
    "\n",
    "### Final Takeaway\n",
    "\n",
    "Political bias classification is an inherently challenging task with significant fairness implications. While our models achieved ~58% accuracy on 3-class classification and ~69% on binary classification, the fairness analysis reveals concerning asymmetric biases particularly in how models confuse left-leaning articles for right-leaning ones more often than the reverse. \n",
    "\n",
    "The binary classification results suggest that distinguishing political extremes (left vs right) is more feasible than identifying the nuanced \"center\" category. Future work should focus on larger datasets, fine-tuned language models, counterfactual fairness testing, and more sophisticated feature engineering to improve both performance and fairness on this important task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
