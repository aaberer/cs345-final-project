{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16cde24a",
   "metadata": {},
   "source": [
    "# CS345 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae5bec2",
   "metadata": {},
   "source": [
    "### Project Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0804b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed4dc40",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "335a10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_news_dataset(dataset_name: str):\n",
    "    base_path = os.getcwd() + '/data'\n",
    "    dataset_path = Path(base_path) / dataset_name\n",
    "    if not dataset_path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset path not found: {dataset_path}\")\n",
    "\n",
    "    # load corpus.tsv\n",
    "    corpus_path = dataset_path / \"corpus.tsv\"\n",
    "    df = pd.read_csv(corpus_path, sep=\"\\t\")\n",
    "    \n",
    "    # load splits\n",
    "    splits_path = dataset_path / \"splits.json\"\n",
    "    splits = {}\n",
    "    if splits_path.exists():\n",
    "        with open(splits_path, \"r\") as f:\n",
    "            splits = json.load(f)\n",
    "\n",
    "    # load feature JSONs from features directory\n",
    "    features_dir = dataset_path / \"features\"\n",
    "    feature_data = {}\n",
    "    if features_dir.exists():\n",
    "        for json_file in features_dir.glob(\"*.json\"):\n",
    "            feature_name = json_file.stem\n",
    "            with open(json_file, \"r\") as f:\n",
    "                feature_data[feature_name] = json.load(f)\n",
    "    \n",
    "    return df, splits, feature_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d694f75",
   "metadata": {},
   "source": [
    "### ACL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54b318e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACL2020 Dataset:\n",
      "Articles: 859\n",
      "Columns: ['source_url', 'source_url_normalized', 'ref', 'fact', 'bias']\n",
      "Available features: ['youtube_subs', 'has_twitter', 'has_wikipedia', 'twitter_profile', 'youtube_numerical', 'youtube_fulltext', 'has_youtube', 'twitter_followers', 'has_facebook', 'articles_body_bert', 'youtube_nela', 'youtube_opensmile', 'articles_title_bert', 'wikipedia_content']\n",
      "Split sizes: train=687 dev=0 test=172\n",
      "(859, 5)\n"
     ]
    }
   ],
   "source": [
    "acl_df, acl_splits, acl_features = load_news_dataset(\"acl2020\")\n",
    "\n",
    "acl_split_data = acl_splits.get('0', {})\n",
    "acl_train_split_size = len(acl_split_data.get('train', []))\n",
    "acl_dev_split_size = len(acl_split_data.get('dev', []))\n",
    "acl_test_split_size = len(acl_split_data.get('test', []))\n",
    "\n",
    "print(\"ACL2020 Dataset:\")\n",
    "print(f\"Articles: {len(acl_df)}\")\n",
    "print(f\"Columns: {list(acl_df.columns)}\")\n",
    "print(f\"Available features: {list(acl_features.keys())}\")\n",
    "print(f\"Split sizes: train={acl_train_split_size} dev={acl_dev_split_size} test={acl_test_split_size}\")\n",
    "print(acl_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1efa0a",
   "metadata": {},
   "source": [
    "### EMNLP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12278a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNLP Dataset:\n",
      "Articles: 1066\n",
      "Columns: ['source_url', 'source_url_normalized', 'ref', 'fact', 'bias']\n",
      "Available features: ['twitter_urlmatch', 'has_twitter', 'wikipedia_toc', 'has_wikipedia', 'twitter_description', 'twitter_created_at', 'twitter_haslocation', 'articles_body_glove', 'twitter_engagement', 'twitter_verified', 'url_structure', 'articles_title_glove', 'alexa', 'wikipedia_categories', 'wikipedia_summary', 'wikipedia_content']\n",
      "Split sizes: train=851 dev=0 test=215\n",
      "(1066, 5)\n"
     ]
    }
   ],
   "source": [
    "emnlp_df, emnlp_splits, emnlp_features = load_news_dataset(\"emnlp18\")\n",
    "\n",
    "emnlp_split_data = emnlp_splits.get('0', {})\n",
    "emnlp_train_split_size = len(emnlp_split_data.get('train', []))\n",
    "emnlp_dev_split_size = len(emnlp_split_data.get('dev', []))\n",
    "emnlp_test_split_size = len(emnlp_split_data.get('test', []))\n",
    "\n",
    "print(\"EMNLP Dataset:\")\n",
    "print(f\"Articles: {len(emnlp_df)}\")\n",
    "print(f\"Columns: {list(emnlp_df.columns)}\")\n",
    "print(f\"Available features: {list(emnlp_features.keys())}\")\n",
    "print(f\"Split sizes: train={emnlp_train_split_size} dev={emnlp_dev_split_size} test={emnlp_test_split_size}\")\n",
    "print(emnlp_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
